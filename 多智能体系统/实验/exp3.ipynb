{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello, I am JYC. This notebook implements all the sub-tasks in experimental topic 3. I will explain the logic behind each concept to assist students who are not proficient in coding to quickly grasp the key points and complete the experiment.\n",
    "\n",
    "This part is rely on a multi-agent simulation library **agentsim** which is developed by myself. You should install agentsim first following [here](https://github.com/Kami-chanw/agentsim).\n",
    "\n",
    "In this experiment, I implemented both a centralized coordination method and a multi-agent reinforcement learning (MARL) method. The first scenario is based on maze cleaning, while the second scenario is based on the Prisoner's Dilemma game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentsim.maze import MazeWindow, MazeCleanEnv\n",
    "from agentsim.pdgame import PDGameEnv, PDGameWindow\n",
    "\n",
    "import pyglet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centralized Coordination Method\n",
    "\n",
    "In the maze cleaning environment, agents are tasked with exploring and cleaning a maze as efficiently as possible. The maze is represented as an undirected, unweighted, acyclic graph, where nodes represent locations and edges represent paths between locations. It should be implemented with IPPO, but I don't have much time to learn it...\n",
    "\n",
    "The primary objectives of this environment are:\n",
    "- To ensure that each agent explores and cleans as many new locations as possible.\n",
    "- To minimize the overlap of cleaned locations between agents to avoid redundant cleaning.\n",
    "- To optimize the overall cleaning time by efficiently coordinating the agents' movements to cover the entire maze.\n",
    "\n",
    "Agents receive rewards based on their actions:\n",
    "- A reward for exploring a new location.\n",
    "- A penalty for revisiting an already cleaned location.\n",
    "- A significant reward for completing the cleaning of the entire maze.\n",
    "\n",
    "The centralized coordination method involves a central controller that directs the actions of all agents, which is totally based on search algorithm. The controller has a global view of the maze and the status of all agents, allowing it to make informed decisions to achieve the objectives efficiently. \n",
    "\n",
    "Its algorithmic process is as follows:\n",
    "\n",
    "1. Perform spectral decomposition on the graph composed of feasible paths in the maze, resulting in `n_agents` subgraphs.\n",
    "2. For each subgraph, arbitrarily select a leaf node `u`, then use breadth-first search (BFS) to find the node `v` that is the furthest from `u`, and set `v` as the starting node.\n",
    "3. Start from `v` to traverse the entire graph, preprocessing the height of each node. The height here is defined as the maximum height of all child nodes plus one. Specifically, the height of a leaf node is 0.\n",
    "4. Continue traversing the entire graph from `v`. When a node `w` has a degree greater than 2 (indicating multiple branches), prioritize the branch with the smaller height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MazeCleanEnv(5, 5, 5)\n",
    "window = MazeWindow(\n",
    "    env,\n",
    "    width=1000,\n",
    "    height=800,\n",
    "    caption=\"Maze Cleaner Simulation\",\n",
    "    solve_interval=0.5,\n",
    ")\n",
    "\n",
    "pyglet.app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Agent Reinforcement Learning Method\n",
    "\n",
    "#### Prisoner's Dilemma Game Environment\n",
    "\n",
    "In the Prisoner's Dilemma (PD) game environment, multiple agents interact with each other in a series of two-player games. Each game round involves two agents making a choice: to cooperate (pay a coin) or to defect (not pay a coin). The outcome of their choices is determined by a reward matrix:\n",
    "\n",
    "- If both players cooperate, they both receive a reward.\n",
    "- If one cooperates and the other defects, the defector receives a higher reward, while the cooperator receives nothing.\n",
    "- If both defect, neither receives a reward.\n",
    "\n",
    "The PD game environment is characterized by the dilemma that while mutual cooperation yields the highest collective reward, individual rationality often leads to defection, resulting in suboptimal outcomes for both players.\n",
    "\n",
    "#### Reinforcement Learning Player: Q-Learner\n",
    "\n",
    "In this environment, a simple reinforcement learning player called the Q-Learner is employed. The Q-Learner's strategy is based on Q-Learning, a model-free reinforcement learning algorithm. Q-Learning works as follows:\n",
    "\n",
    "- **State Representation**: The state can be represented by the current strategy of the opponent (e.g., frequency of cooperation and defection).\n",
    "- **Action Space**: The possible actions are to cooperate or defect.\n",
    "- **Reward Function**: The reward received depends on the outcomes of the PD game rounds.\n",
    "- **Q-Table**: The Q-Learner maintains a Q-table where each state-action pair has an associated Q-value, representing the expected future rewards of taking that action in that state.\n",
    "- **Update Rule**: After each game round, the Q-Learner updates its Q-values using the formula:\n",
    "  $$\n",
    "  Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]\n",
    "  $$\n",
    "  where $s$ is the current state, $a$ is the action taken, $r$ is the reward received, $s'$ is the next state, $\\alpha$ is the learning rate, and $\\gamma$ is the discount factor.\n",
    "- **Policy**: The Q-Learner follows an $\\varepsilon$-greedy policy, where with probability $\\varepsilon$ it chooses a random action (exploration), and with probability $1-\\varepsilon$ it chooses the action with the highest Q-value (exploitation).\n",
    "\n",
    "By using the Q-Learner in the PD game environment, we can explore how individual agents learn to cooperate or defect based on their experiences and how their strategies evolve over time through repeated interactions. This setup allows for the analysis of emergent behaviors and the effectiveness of reinforcement learning in multi-agent settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_matrix = [[(2, 2), (-1, 3)], [(3, -1), (0, 0)]]\n",
    "\n",
    "role_num_dict = {\"copycat\": 5, \"cooperator\": 5, \"fraud\": 5, \"grudger\": 5, \"qlearner\": 20}\n",
    "env = PDGameEnv(reward_matrix, role_num_dict, 5)\n",
    "window = PDGameWindow(env)\n",
    "pyglet.app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
